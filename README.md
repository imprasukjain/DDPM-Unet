# Denoising Diffusion Probabilistic Models (DDPM)

A PyTorch/TensorFlow implementation of the groundbreaking paper ["Denoising Diffusion Probabilistic Models"](https://arxiv.org/abs/2006.11239) by Ho et al. (2020), trained on the CelebA dataset to generate realistic human faces.

## Table of Contents
- [Overview](#overview)
- [The Mathematics Behind DDPM](#the-mathematics-behind-ddpm)
- [U-Net Architecture](#u-net-architecture)
- [Training Process](#training-process)
- [Results](#results)
- [Installation & Usage](#installation--usage)

---

## Overview

Denoising Diffusion Probabilistic Models (DDPM) are a class of generative models that learn to generate data by reversing a gradual noising process. The key insight is beautifully simple yet powerful: if we can learn to denoise images step-by-step, we can generate new images by starting from pure noise and iteratively denoising it.

### The Core Idea

**Forward Process (Diffusion):** Gradually add Gaussian noise to real images over T timesteps until they become pure noise.

**Reverse Process (Denoising):** Learn a neural network to reverse this process removing noise step by step to generate realistic images from random noise.

---

## The Mathematics Behind DDPM

### Forward Diffusion Process

The forward process adds noise to an image xâ‚€ over T timesteps. At each timestep t, we add a small amount of Gaussian noise:

```
q(xâ‚œ | xâ‚œâ‚‹â‚) = N(xâ‚œ; âˆš(1-Î²â‚œ)Â·xâ‚œâ‚‹â‚, Î²â‚œI)
```

Where:
- **xâ‚œ**: The noisy image at timestep t
- **Î²â‚œ**: The noise schedule (variance) at timestep t
- **N**: Gaussian distribution

**Key Insight:** We can derive a closed-form expression to jump directly to any timestep t without computing all intermediate steps!

### The Magic of Î±â‚œ and á¾±â‚œ (Alpha Bar)

This is where most explanations get hand-wavy, but the math is crucial:

Define:
```
Î±â‚œ = 1 - Î²â‚œ
```

**Why Î±â‚œ?** It represents the "signal retention rate" at timestep t. If Î²â‚œ is the noise we add, then Î±â‚œ = 1 - Î²â‚œ is how much of the original signal we keep.

Now, the cumulative product (this is the key):
```
á¾±â‚œ = âˆ(i=1 to t) Î±áµ¢ = Î±â‚ Â· Î±â‚‚ Â· Î±â‚ƒ Â· ... Â· Î±â‚œ
```

**Why á¾±â‚œ (alpha bar)?** This represents the **cumulative signal retention** from the original image xâ‚€ to timestep t. 

### The Reparameterization Trick

Using á¾±â‚œ, we can express any noisy image xâ‚œ directly in terms of the original image xâ‚€:

```
xâ‚œ = âˆšá¾±â‚œ Â· xâ‚€ + âˆš(1-á¾±â‚œ) Â· Îµ

where Îµ ~ N(0, I) is standard Gaussian noise
```

**This is huge!** Instead of applying noise T times sequentially, we can jump to any timestep in one step.

**Why these specific coefficients?**
- **âˆšá¾±â‚œ**: Scales the original image
- **âˆš(1-á¾±â‚œ)**: Scales the noise
- Together they ensure the variance remains normalized: `Var(xâ‚œ) = á¾±â‚œ + (1-á¾±â‚œ) = 1`

### The Training Objective

The model learns to predict the noise Îµ that was added to create xâ‚œ from xâ‚€:

```
L_simple = ð”¼[â€–Îµ - Îµ_Î¸(xâ‚œ, t)â€–Â²]
```

Where:
- **Îµ**: The actual noise added
- **Îµ_Î¸(xâ‚œ, t)**: The noise predicted by our neural network
- **t**: The timestep (fed as input to help the model know how much noise to expect)

**Why predict noise instead of the clean image?** 
1. More stable training (noise has consistent scale)
2. Better gradient flow
3. Empirically works better across different timesteps

### Reverse Process (Sampling)

To generate images, we start from pure noise x_T ~ N(0, I) and iteratively denoise:

```
xâ‚œâ‚‹â‚ = 1/âˆšÎ±â‚œ Â· (xâ‚œ - (1-Î±â‚œ)/âˆš(1-á¾±â‚œ) Â· Îµ_Î¸(xâ‚œ, t)) + Ïƒâ‚œ Â· z

where z ~ N(0, I) and Ïƒâ‚œ = âˆšÎ²â‚œ
```

We repeat this for t = T, T-1, ..., 1 to get our final image xâ‚€.

---

## U-Net Architecture

We use a modified U-Net architecture as our denoising network Îµ_Î¸. The U-Net was originally designed for image segmentation but works exceptionally well for diffusion models.

### Why U-Net?

1. **Preserves spatial information** through skip connections
2. **Multi-scale processing** via downsampling and upsampling
3. **Bottleneck captures global context** while maintaining local details

### Architecture Overview

```
Input: Noisy image xâ‚œ (64Ã—64Ã—3) + Timestep embedding t

                        [Encoder - Downsampling Path]
                                    â†“
    64Ã—64Ã—32  â†’  [ResBlock Ã— 2] â†’ [Skip Connection 1] â”€â”
                      â†“ MaxPool(2Ã—2)                     â”‚
    32Ã—32Ã—64  â†’  [ResBlock Ã— 2] â†’ [Attention] â†’ [Skip 2]â”€â”¤
                      â†“ MaxPool(2Ã—2)                      â”‚
    16Ã—16Ã—128 â†’  [ResBlock Ã— 2] â†’ [Skip Connection 3] â”€â”¤ â”‚
                      â†“ MaxPool(2Ã—2)                     â”‚ â”‚
    8Ã—8Ã—256   â†’  [ResBlock Ã— 2] â†’ [Skip Connection 4] â”€â”¤ â”‚ â”‚
                                                         â”‚ â”‚ â”‚
                      [Bottleneck]                       â”‚ â”‚ â”‚
    8Ã—8Ã—256   â†’  [ResBlock] â†’ [Attention] â†’ [ResBlock] â”‚ â”‚ â”‚
                                                         â”‚ â”‚ â”‚
                        [Decoder - Upsampling Path]     â”‚ â”‚ â”‚
                                    â†“                    â”‚ â”‚ â”‚
    8Ã—8Ã—256   â†’  [Concatenate Skip 4] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
                      â†“ Upsample(2Ã—2)                      â”‚ â”‚
    16Ã—16Ã—128 â†’  [ResBlock Ã— 2] â†’ [Concatenate Skip 3] â†â”€â”€â”˜ â”‚
                      â†“ Upsample(2Ã—2)                        â”‚
    32Ã—32Ã—64  â†’  [ResBlock Ã— 2] â†’ [Attention] â†’ [Concat Skip 2] â†â”˜
                      â†“ Upsample(2Ã—2)
    64Ã—64Ã—32  â†’  [ResBlock Ã— 2] â†’ [Concatenate Skip 1] â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
    64Ã—64Ã—3   â†’  [Final Conv] â†’ Output: Predicted noise Îµ

Timestep Embedding: Sinusoidal positional encoding (1024-dim) 
                   injected into each ResBlock via AdaGN
```

### Key Components Explained

#### 1. **Residual Blocks (ResBlock)**
```
Input â†’ GroupNorm â†’ SiLU â†’ Conv3Ã—3 â†’ 
     â†’ GroupNorm â†’ Add Timestep Embedding â†’ 
     â†’ SiLU â†’ Conv3Ã—3 â†’ Add Input (Residual) â†’ Output
```

**Why?**
- Gradient flow through skip connections
- Timestep conditioning at each layer
- Stable training for deep networks

#### 2. **Timestep Embedding**
The timestep t is encoded using sinusoidal positional embeddings (like in Transformers):

```python
pos = t / (10000 ^ (2i/d))
emb[2i] = sin(pos)
emb[2i+1] = cos(pos)
```

This creates a unique embedding for each timestep that the model learns to interpret.

#### 3. **Self-Attention Layers**
Applied at 32Ã—32 resolution (after first downsampling):

```
Q, K, V = Linear(x)
Attention(Q,K,V) = Softmax(QK^T / âˆšd) Â· V
```

**Why only at 32Ã—32?**
- Computational efficiency (attention is O(nÂ²))
- Captures mid-level features (edges, textures)
- Still provides global context without being too expensive

#### 4. **Downsampling Path**
- **Purpose**: Extract hierarchical features from coarse to fine
- **Operations**: 
  - ResBlocks process features at each resolution
  - MaxPooling(2Ã—2) reduces spatial dimensions
  - Feature channels double at each level (32â†’64â†’128â†’256)

#### 5. **Upsampling Path**
- **Purpose**: Reconstruct spatial resolution while refining features
- **Operations**:
  - Bilinear upsampling (2Ã—2) increases spatial dimensions
  - Concatenate with corresponding encoder features (skip connections)
  - ResBlocks process the combined features
  - Feature channels halve at each level (256â†’128â†’64â†’32)

#### 6. **Skip Connections**
These are **critical** and often misunderstood:

**What they do:** Concatenate encoder features directly to decoder features at matching resolutions.

**Why they matter:**
- **Preserve fine details** that get lost in downsampling
- **Better gradient flow** during backpropagation
- **Multi-scale information:** Decoder sees both:
  - High-level features from the bottleneck (what to generate)
  - Low-level features from encoder (where to place details)

**Example at 32Ã—32 resolution:**
```
Encoder output: [Batch, 32, 32, 64]
Decoder before concat: [Batch, 32, 32, 64]
After concatenation: [Batch, 32, 32, 128]  â† Double the channels!
```

### Downsampling vs Upsampling: The Information Flow

**Downsampling (Encoder):**
```
64Ã—64Ã—3  â†’ What is this? (Raw pixels)
32Ã—32Ã—64 â†’ Edges and basic shapes
16Ã—16Ã—128 â†’ Object parts (eyes, nose)
8Ã—8Ã—256 â†’ Global structure (face composition)
```

**Bottleneck:**
```
8Ã—8Ã—256 â†’ Semantic understanding (it's a face, lighting, pose)
```

**Upsampling (Decoder):**
```
8Ã—8Ã—256 â†’ Start with global structure
16Ã—16Ã—128 â†’ Add object parts + encoder details
32Ã—32Ã—64 â†’ Refine textures + encoder edges  
64Ã—64Ã—3 â†’ Final details + encoder pixel-level info
```

**The Magic:** At each upsampling step, the network combines:
- **Top-down information:** "What should be generated" (from bottleneck)
- **Bottom-up information:** "Where details should go" (from skip connections)

This is why U-Net works so well for denoising it can maintain spatial precision while understanding global context!

---

## Training Process

### Hyperparameters

```python
BATCH_SIZE = 16  # Using P100 GPU
TIME_STEPS = 1000
IMAGE_SIZE = 64Ã—64Ã—3
LEARNING_RATE = 2e-4
N_EPOCHS = 10

# U-Net Architecture
N_RESNETS = 2  # ResBlocks per level
N_GROUPS = 2   # Group Normalization groups
N_HEADS = 8    # Multi-head attention
ATTN_DIM = 256 # Attention dimension

# Noise Schedule (Linear)
Î²_start = 1e-4
Î²_end = 0.02
Î²â‚œ = linspace(Î²_start, Î²_end, TIME_STEPS)
```

### Training Algorithm

```
For each epoch:
    For each batch of images xâ‚€:
        1. Sample timestep t ~ Uniform(1, T)
        2. Sample noise Îµ ~ N(0, I)
        3. Create noisy image: xâ‚œ = âˆšá¾±â‚œÂ·xâ‚€ + âˆš(1-á¾±â‚œ)Â·Îµ
        4. Predict noise: Îµ_pred = model(xâ‚œ, t)
        5. Compute loss: L = MSE(Îµ, Îµ_pred)
        6. Backprop and update weights
```

### GPU Performance Analysis

We conducted extensive testing to optimize training performance across different GPU configurations:

#### T4 vs P100 Comparison

| Metric | T4 GPU | P100 GPU | Improvement |
|--------|--------|----------|-------------|
| **Time per Epoch** | 13,672 sec (~3.8 hrs) | 6,972 sec (~1.94 hrs) | **1.95x faster** |
| **Batch Size** | 8 | 16 | 2x larger |
| **Memory Bandwidth** | 320 GB/s | 732 GB/s | 2.3x faster |
| **FP16 Performance** | 65 TFLOPS | 21 TFLOPS | Better FP32 |
| **Training Stability** | Frequent OOM errors | Stable training | âœ… |

#### Why P100 is Superior for DDPM Training

**1. Memory Bandwidth (Critical for U-Net)**
- **T4**: 320 GB/s - bottleneck for large feature maps
- **P100**: 732 GB/s - 2.3x faster data movement
- U-Net's skip connections require heavy memory I/O
- **Result**: P100 handles concatenations and upsampling much faster

**2. Batch Size Capability**
- **T4**: Limited to batch_size=8
  - Frequent OOM (Out of Memory) errors with batch_size=16
  - Required aggressive memory management
- **P100**: Stable with batch_size=16
  - Better GPU utilization
  - More stable gradient estimates
  - Faster convergence

**3. Mixed Precision Training**
```python
# Mixed precision is crucial for memory optimization
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
```
- **T4**: Excels at FP16 (65 TFLOPS) but limited by memory bandwidth
- **P100**: Better balanced FP32 performance with superior memory bandwidth
- **Key insight**: For DDPM, memory bandwidth > raw FLOPS

**4. Computational Efficiency**
- T4's tensor cores are optimized for inference
- P100's architecture is optimized for training workloads
- The 1.95x speedup validates P100's training superiority

### Memory Optimization Techniques Applied

#### Issue: Initial OOM Errors on T4
```
ResourceExhaustedError: failed to allocate memory
```

#### Solutions Implemented:

**1. Enable Memory Growth**
```python
gpus = tf.config.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
```
**Impact**: Allocates memory as needed vs. pre-allocating all GPU memory

**2. Mixed Precision Training**
```python
from tensorflow.keras import mixed_precision
policy = mixed_precision.Policy('mixed_float16')
mixed_precision.set_global_policy(policy)
```
**Impact**: ~50% memory reduction + faster computation

**3. Batch Size Tuning**
- Started with batch_size=16 â†’ OOM on T4
- Reduced to batch_size=8 â†’ Stable on T4
- Switched to P100 â†’ batch_size=16 works perfectly

**4. Single GPU Training Decision**
- Initially attempted multi-GPU (2x T4) with MirroredStrategy
- Encountered complex errors:
  ```
  InvalidArgumentError: You must feed a value for placeholder tensor
  ValueError: A KerasTensor cannot be used as input to a TensorFlow function
  ```
- **Decision**: Single GPU training is more stable and easier to debug
- **Trade-off**: Sacrificed potential 1.8-1.9x speedup for reliability

### Training Stability: The Extract Function Bug

**Critical Bug Found:**
```python
# WRONG - Causes shape mismatch in distributed training
def extract(a, t, x_shape):
    b, *_ = t.shape  # Fails in graph mode!
    out = tf.gather(a, t)
    return tf.reshape(out, (b,*((1,)*(len(x_shape)-1))))
```

**Problem**: `t.shape` returns symbolic dimensions in graph mode, causing:
```
ValueError: required broadcastable shapes [Op:Sub]
```

**Fix:**
```python
# CORRECT - Works in both eager and graph mode
def extract(a, t, x_shape):
    batch_size = tf.shape(t)[0]  # Dynamic shape
    out = tf.gather(a, t)
    return tf.reshape(out, [batch_size, 1, 1, 1])  # List, not tuple
```

**Key Lesson**: Always use `tf.shape()` for dynamic dimensions in `@tf.function` decorated code!

### Why This Works

The model learns to denoise images at **all** noise levels simultaneously:
- Early timesteps (t near 0): Tiny noise, subtle corrections
- Middle timesteps: Moderate noise, shape refinement  
- Late timesteps (t near T): Heavy noise, high-level structure

By training on random timesteps, the model becomes robust across the entire diffusion process.

### Final Training Configuration

```python
# Optimized for P100 GPU
DEVICE = "P100"
BATCH_SIZE = 16
MIXED_PRECISION = True
MEMORY_GROWTH = True
SINGLE_GPU = True  # More stable than distributed

# Expected Training Time
TOTAL_EPOCHS = 10
TIME_PER_EPOCH = 6972  # seconds
TOTAL_TIME = 10 * 6972 / 3600  # â‰ˆ 19.4 hours
```

---

## Results

### Training Metrics
- **Dataset**: CelebA (202,599 images)
- **GPU**: NVIDIA P100 (16GB)
- **Training Time**: 
  - Per Epoch: 6,972 seconds (~1.94 hours)
  - Total (10 epochs): ~19.4 hours
- **Batch Size**: 16
- **Mixed Precision**: Enabled (FP16)
- **Final Loss**: [Your final loss here]

### Performance Comparison

#### Hardware Evolution During Training

| Stage | GPU | Batch Size | Time/Epoch | Issue |
|-------|-----|------------|------------|-------|
| **Initial** | T4 | 16 | OOM Error | Memory exhausted |
| **Attempt 1** | T4 | 8 | 13,672 sec | Stable but slow |
| **Attempt 2** | 2x T4 (MirroredStrategy) | 16 | Failed | Distributed training errors |
| **Final** | P100 | 16 | 6,972 sec | âœ… Optimal |

**Key Insight**: P100's superior memory bandwidth (732 GB/s vs 320 GB/s) makes it 1.95x faster than T4 for U-Net architectures with heavy skip connections.

### Generated Samples
[Add your generated images here after training]

**Sampling Process:**
1. Start with pure noise: x_T ~ N(0, I)
2. Iteratively denoise for t = 1000, 999, ..., 1
3. Each step removes a small amount of predicted noise
4. Final result: Realistic 64Ã—64 face image

### Sampling Process Visualization
[Show the gradual denoising from Tâ†’0]

**Timestep Examples:**
- t=1000 (pure noise): Random static
- t=750: Vague color blobs
- t=500: Face shape emerges
- t=250: Features become clear
- t=0: High-quality face

### Training Loss Curve
[Add your loss curve plot here]

Expected behavior:
- Initial loss: ~0.5-1.0 (model learning noise patterns)
- Mid training: ~0.1-0.3 (refining details)
- Final loss: ~0.05-0.15 (generating realistic faces)

---

## Installation & Usage

### Requirements
```bash
pip install tensorflow tensorflow-gpu numpy matplotlib
```

### Training
```python
# Load dataset
python train.py --dataset celeba --epochs 10 --batch_size 8

# Resume from checkpoint
python train.py --resume unet_epoch_5.keras
```

### Generation
```python
# Generate new faces
python generate.py --num_samples 16 --model unet_epoch_10.keras
```

---

## Key Takeaways

### Mathematical Insights
1. **á¾±â‚œ (alpha bar) is the cumulative signal retention**, allowing us to jump to any noise level directly
2. **The model predicts noise, not images**, which provides more stable training
3. **The variance-preserving property** (âˆšá¾±â‚œ and âˆš(1-á¾±â‚œ)) ensures stable noise addition at all timesteps
4. **Timestep conditioning** is essential the model needs to know how much noise to remove
5. **The reparameterization trick** enables efficient one-step computation: xâ‚œ = âˆšá¾±â‚œÂ·xâ‚€ + âˆš(1-á¾±â‚œ)Â·Îµ

### Architectural Insights
6. **U-Net's skip connections** preserve spatial details while the bottleneck captures global context
7. **Multi-scale attention** at 32Ã—32 balances computational cost with global understanding
8. **ResBlocks with timestep injection** allow the model to adapt its denoising strategy per timestep
9. **Symmetric encoder-decoder** ensures information can flow both ways

### Training Insights
10. **P100 > T4 for training**: Memory bandwidth matters more than raw FLOPS for U-Net
11. **Single GPU is more stable** than distributed training for complex models with dynamic shapes
12. **Mixed precision training** is essential saves 50% memory with minimal accuracy loss
13. **The `extract()` function bug** taught us: always use `tf.shape()` for dynamic dimensions
14. **Batch size impacts stability**: Larger batches = better gradient estimates = faster convergence

### Implementation Lessons Learned

**Problem**: Multi-GPU training with MirroredStrategy
```python
# Attempted but failed due to:
InvalidArgumentError: placeholder tensor errors
ValueError: KerasTensor cannot be used in TensorFlow functions
```
**Solution**: Single GPU training is simpler, more stable, and still fast enough

**Problem**: T4 OOM with batch_size=16
```python
ResourceExhaustedError: failed to allocate memory
```
**Solution**: Either reduce batch_size=8 OR switch to P100

**Problem**: Shape broadcasting errors in distributed training
```python
# Bug in extract function
b, *_ = t.shape  # Symbolic shape breaks in graph mode
```
**Solution**: Use `tf.shape(t)[0]` for dynamic batch dimensions

### Why DDPM Works So Well

The genius of DDPM lies in its simplicity:
- **Training**: Predict noise at random timesteps (parallelizable, stable)
- **Sampling**: Iteratively denoise (slow but high quality)
- **Architecture**: U-Net naturally fits the task (spatial preservation + global context)
- **Math**: Variance-preserving noise addition keeps everything normalized

---

## References

1. [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) - Ho et al., 2020
2. [Understanding Diffusion Models](https://arxiv.org/abs/2208.11970) - Luo, 2022
3. [U-Net: Convolutional Networks for Biomedical Image Segmentation](https://arxiv.org/abs/1505.04597) - Ronneberger et al., 2015

---

## License

MIT License - Feel free to use this code for your own projects!

---

## Acknowledgments

- Original DDPM paper authors for the groundbreaking research
- CelebA dataset creators
- The open-source community for TensorFlow/PyTorch implementations

---

**Star this repo if you found it helpful!**
